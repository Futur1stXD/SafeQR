{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395300</th>\n",
       "      <td>englishclub.com/english-language-history.htm</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210875</th>\n",
       "      <td>en.wikipedia.org/wiki/1979_Ottawa_Rough_Riders...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161548</th>\n",
       "      <td>whmc.umsystem.edu/exhibits/ramsay/ramsay_crawf...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86167</th>\n",
       "      <td>spoke.com/info/pC7sG0/JonWolter</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43025</th>\n",
       "      <td>acronyms.thefreedictionary.com/South+African+E...</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url    type\n",
       "395300       englishclub.com/english-language-history.htm  benign\n",
       "210875  en.wikipedia.org/wiki/1979_Ottawa_Rough_Riders...  benign\n",
       "161548  whmc.umsystem.edu/exhibits/ramsay/ramsay_crawf...  benign\n",
       "86167                     spoke.com/info/pC7sG0/JonWolter  benign\n",
       "43025   acronyms.thefreedictionary.com/South+African+E...  benign"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/malicious.csv')\n",
    "\n",
    "benign_subset = df[df['type'] == 'benign'].sample(frac=0.25)\n",
    "malicious_subset = df[(df['type'] == 'malware') | (df['type'] == 'defacement')]\n",
    "benign_subset['type'] = benign_subset['type'].replace({'benign': 'good'})\n",
    "\n",
    "df = pd.concat([benign_subset, malicious_subset])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "\n",
    "df['text_tokenized'] = df.url.map(lambda t: tokenizer.tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "df['text_stemmed'] = df['text_tokenized'].map(lambda l: [stemmer.stem(word = word) for word in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395300</th>\n",
       "      <td>englishclub.com/english-language-history.htm</td>\n",
       "      <td>benign</td>\n",
       "      <td>[englishclub, com, english, language, history,...</td>\n",
       "      <td>[englishclub, com, english, languag, histori, ...</td>\n",
       "      <td>englishclub com english languag histori htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210875</th>\n",
       "      <td>en.wikipedia.org/wiki/1979_Ottawa_Rough_Riders...</td>\n",
       "      <td>benign</td>\n",
       "      <td>[en, wikipedia, org, wiki, Ottawa, Rough, Ride...</td>\n",
       "      <td>[en, wikipedia, org, wiki, ottawa, rough, ride...</td>\n",
       "      <td>en wikipedia org wiki ottawa rough rider season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161548</th>\n",
       "      <td>whmc.umsystem.edu/exhibits/ramsay/ramsay_crawf...</td>\n",
       "      <td>benign</td>\n",
       "      <td>[whmc, umsystem, edu, exhibits, ramsay, ramsay...</td>\n",
       "      <td>[whmc, umsystem, edu, exhibit, ramsay, ramsay,...</td>\n",
       "      <td>whmc umsystem edu exhibit ramsay ramsay crawfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86167</th>\n",
       "      <td>spoke.com/info/pC7sG0/JonWolter</td>\n",
       "      <td>benign</td>\n",
       "      <td>[spoke, com, info, pC, sG, JonWolter]</td>\n",
       "      <td>[spoke, com, info, pc, sg, jonwolt]</td>\n",
       "      <td>spoke com info pc sg jonwolt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43025</th>\n",
       "      <td>acronyms.thefreedictionary.com/South+African+E...</td>\n",
       "      <td>benign</td>\n",
       "      <td>[acronyms, thefreedictionary, com, South, Afri...</td>\n",
       "      <td>[acronym, thefreedictionari, com, south, afric...</td>\n",
       "      <td>acronym thefreedictionari com south african eq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url    type  \\\n",
       "395300       englishclub.com/english-language-history.htm  benign   \n",
       "210875  en.wikipedia.org/wiki/1979_Ottawa_Rough_Riders...  benign   \n",
       "161548  whmc.umsystem.edu/exhibits/ramsay/ramsay_crawf...  benign   \n",
       "86167                     spoke.com/info/pC7sG0/JonWolter  benign   \n",
       "43025   acronyms.thefreedictionary.com/South+African+E...  benign   \n",
       "\n",
       "                                           text_tokenized  \\\n",
       "395300  [englishclub, com, english, language, history,...   \n",
       "210875  [en, wikipedia, org, wiki, Ottawa, Rough, Ride...   \n",
       "161548  [whmc, umsystem, edu, exhibits, ramsay, ramsay...   \n",
       "86167               [spoke, com, info, pC, sG, JonWolter]   \n",
       "43025   [acronyms, thefreedictionary, com, South, Afri...   \n",
       "\n",
       "                                             text_stemmed  \\\n",
       "395300  [englishclub, com, english, languag, histori, ...   \n",
       "210875  [en, wikipedia, org, wiki, ottawa, rough, ride...   \n",
       "161548  [whmc, umsystem, edu, exhibit, ramsay, ramsay,...   \n",
       "86167                 [spoke, com, info, pc, sg, jonwolt]   \n",
       "43025   [acronym, thefreedictionari, com, south, afric...   \n",
       "\n",
       "                                                text_sent  \n",
       "395300        englishclub com english languag histori htm  \n",
       "210875    en wikipedia org wiki ottawa rough rider season  \n",
       "161548  whmc umsystem edu exhibit ramsay ramsay crawfo...  \n",
       "86167                        spoke com info pc sg jonwolt  \n",
       "43025   acronym thefreedictionari com south african eq...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_sent'] = df['text_stemmed'].map(lambda l: ' '.join(l))\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "feature = cv.fit_transform(df.text_sent)\n",
    "\n",
    "feature[:5].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(feature, df.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9941865392111998"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(trainX, trainY)\n",
    "\n",
    "lr.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9667124286029051"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(trainX, trainY)\n",
    "\n",
    "mnb.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9941526414806529"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_ls = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize, stop_words = 'english'), LogisticRegression())\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(df.url, df.type)\n",
    "\n",
    "pipeline_ls.fit(trainX, trainY)\n",
    "\n",
    "pipeline_ls.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pipeline_ls, open('../models/malware-defacement.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['benign' 'benign' 'malware' 'benign']\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open('../models/malware-defacement.pkl', 'rb'))\n",
    "\n",
    "test = ['kaspi.kz', 'avtobys.kz', 'rutracker.ru/mal.exe', 'youtube.com']\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "tokenized = [tokenizer.tokenize(t) for t in test]\n",
    "stemmed = [[stemmer.stem(word=word) for word in l] for l in tokenized]\n",
    "sent = [' '.join(l) for l in stemmed] \n",
    "\n",
    "print(loaded_model.predict(sent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
